{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf - By Tomas Mikolov,Ilya Sutskever, Kai Chen, Greg Corrado, effrey Dean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Word2Vec is vector representation of word. The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.\n",
    "\n",
    "A simple way to investigate the learned representations is to find the closest words for a user-specified word. The distance tool serves that purpose. For example, if you enter 'france', distance will display the most similar words and their distances to 'france', which should look like:\n",
    "\n",
    "```\n",
    "\n",
    "Word Cosine distance\n",
    "\n",
    "            spain              0.678515\n",
    "          belgium              0.665923\n",
    "      netherlands              0.652428\n",
    "            italy              0.633130\n",
    "      switzerland              0.622323\n",
    "       luxembourg              0.610033\n",
    "         portugal              0.577154\n",
    "           russia              0.571507\n",
    "          germany              0.563291\n",
    "        catalonia              0.534176\n",
    "```\n",
    "\n",
    "There are two main learning algorithms in word2vec : continuous bag-of-words and continuous skip-gram. The switch -cbow allows the user to pick one of these learning algorithms. Both algorithms learn the representation of a word that is useful for prediction of other words in the sentence\n",
    "\n",
    "#### Facts:\n",
    "<li>Predictive method, not a count-based as Glove or BagOfWords</li>\n",
    "<li>architecture: skip-gram (slower, better for infrequent words, use “Negative Sampling”) vs CBOW (fast)</li>\n",
    "<li>the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)</li> \n",
    "<li>sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range \\begin{equation*}10^{-3} \\text{  to  }10^{-5}\\end{equation*}</li>\n",
    "<li>dimensionality of the word vectors: usually more is better, but not always</li>\n",
    "<li>context (window) size: for skip-gram usually around 10, for CBOW around 5</li>\n",
    "<li>google pretrained model exists</li>\n",
    "\n",
    "#### Word2Vec (Gensim) vs Glove (Text2vec)\n",
    "![alt text](http://dsnotes.com/articles/figure/2015-12-01-glove-enwiki-unnamed-chunk-3-1.png)\n",
    "\n",
    "#### Useful links\n",
    "<li>https://www.tensorflow.org/tutorials/word2vec/</li>\n",
    "<li>https://code.google.com/archive/p/word2vec/ </li>\n",
    "<li>https://radimrehurek.com/gensim/models/word2vec.html - gensim usage</li>\n",
    "<li>https://rare-technologies.com/word2vec-tutorial/ - creator of gensim</li>\n",
    "<li>https://ru.wikipedia.org/wiki/Дистрибутивная_семантика</li>\n",
    "<li>https://otvet.mail.ru/question/34643535 </li>\n",
    "<li>http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</li>\n",
    "<li>http://sebastianruder.com/word-embeddings-softmax/index.html#negativesampling</li>\n",
    "<li>http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf</li>\n",
    "<li>http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</li>\n",
    "<li>https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/tensorflow/Tensorflow-5-Word2Vec.ipynb</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
