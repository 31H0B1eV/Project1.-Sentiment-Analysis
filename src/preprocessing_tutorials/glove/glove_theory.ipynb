{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove\n",
    "http://www-nlp.stanford.edu/pubs/glove.pdf – Jeffrey Pennington, Richard Socher, Christopher D. Manning 2014.\n",
    "http://nlp.stanford.edu/projects/glove/ – pre-trained word vectors and some theory\n",
    "\n",
    "\n",
    "### What is GloVe? :) \n",
    "*GloVe* is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "More detailed explanation https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/\n",
    "\n",
    "![alt text](http://building-babylon.net/wp-content/uploads/2016/02/glove-matrix-factorisation-3.jpg)\n",
    "\n",
    "### Difference with word2vec\n",
    "Word2Vec and GloVe learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora). They differ in that word2vec is a \"predictive\" model, whereas GloVe is a \"count-based\" model.\n",
    "\n",
    "Predictive models learn their vectors in order to improve their predictive ability of Loss(target word | context words; Vectors), i.e. the loss of predicting the target words from the context words given the vector representations. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD, etc.\n",
    "\n",
    "Count-based models learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. They first construct a large matrix of (words x context) co-occurrence information, i.e. for each \"word\" (the rows), you count how frequently we see this word in some \"context\" (the columns) in a large corpus.  The number of \"contexts\" is of course large, since it is essentially combinatorial in size. So then they factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a \"reconstruction loss\" which tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data. In the specific case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. This turns out to be A Good Thing in terms of the quality of the learned representations.\n",
    "\n",
    "However, as pointed out, when we control for all the training hyper-parameters, the embeddings generated using the two methods tend to perform very similarly in downstream NLP tasks. The additional benefits of GloVe over word2vec is that it is easier to parallelize the implementation which means it's easier to train over more data, which, with these models, is always A Good Thing.\n",
    "\n",
    "### When to use?\n",
    "Works better with neural nets, such as RNN/LSTM and CNN. On machine learning algorithms a result will be worse than on bag of words or on vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
